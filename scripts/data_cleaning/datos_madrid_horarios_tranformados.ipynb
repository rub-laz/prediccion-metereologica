{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb0f319-8d5d-4086-94e2-35c6c0559eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "\n",
    "\n",
    "# Configuración detallada para Spark\n",
    "conf = SparkConf().setAppName(\"ClusterConfigExample\") \n",
    "\n",
    "# Iniciar el SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Crear la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd93cd1-ed68-497c-aac3-16f0287155a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- temperature_2m: double (nullable = true)\n",
      " |-- relative_humidity_2m: double (nullable = true)\n",
      " |-- dew_point_2m: double (nullable = true)\n",
      " |-- apparent_temperature: double (nullable = true)\n",
      " |-- precipitation_probability: string (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- wind_speed_10m: double (nullable = true)\n",
      " |-- temperature_80m: string (nullable = true)\n",
      " |-- soil_temperature_0cm: string (nullable = true)\n",
      "\n",
      "17568\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('madrid_2020_2022.csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762244ee-c586-47b3-8738-d79140290c72",
   "metadata": {},
   "source": [
    "### Exploratorio\n",
    "- Eliminar las columnas que no se vayan a utilizar.\n",
    "- Cambiar el tipo de dato.\n",
    "- Tranformar las columnas pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c73a258-6824-41c6-adc9-3e404e9b9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe: todas son strings, hay que cambiar el tipo de dato a fecha y numericos\n",
    "# 1. Elimino dichas columnas por ser innecesarias\n",
    "# 2. Cambio el tipo de dato y remplazo ',' por '.'\n",
    "# 3. La columna fecha la convierto a date\n",
    "columnas = [\"_c0\",\"precipitation_probability\",\"temperature_80m\",\"soil_temperature_0cm\",\"rain\",\"dew_point_2m\",\"apparent_temperature\"]\n",
    "df = df.drop(*columnas)\n",
    "for columna in df.columns:\n",
    "    if columna in [\"temperature_2m\",\"relative_humidity_2m\",\"precipitation\",\"wind_speed_10m\"]:\n",
    "        df = df.withColumn(columna, F.regexp_replace(F.col(columna), \",\", \".\").cast(\"double\"))\n",
    "    elif columna == \"date\":\n",
    "        df = df.withColumn(\"date\", F.regexp_replace(F.col(\"date\"), '\\\\+', \"\"))\n",
    "        df = df.withColumn(\"date\", F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4ee76-8098-4284-8426-f18649195ae8",
   "metadata": {},
   "source": [
    "- Columnas finales:\n",
    "    - **Date**: fecha.\n",
    "    - **Temperature_2m**: temperratura a 2 metros del suelo.\n",
    "    - **Relative_humidity_2m**: humedad relativa a 2 metros del suelo.\n",
    "    - **Precipitation**: precipitación.\n",
    "    - **Wind_speed_10m**: velocidad del viento a 10 metros del suelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bab4181-737c-4772-af2b-97cd15ee5489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- temperature_2m: double (nullable = true)\n",
      " |-- relative_humidity_2m: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wind_speed_10m: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39656a48-67dd-4856-980c-85031665ff09",
   "metadata": {},
   "source": [
    "- Genero una lista de días entre dos rangos de fechas para comprobar que no falten fechas en el dataframe original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746b31bf-c053-4190-8bbe-b357902e0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2022, 1, 1)\n",
    "\n",
    "date_range_list = [(start_date + timedelta(days=d, hours=h)).strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "                  for d in range((end_date - start_date).days + 1) for h in range(24)]\n",
    "\n",
    "# Crear un DataFrame en PySpark con las fechas en formato string\n",
    "date_range_df = spark.createDataFrame([(d,) for d in date_range_list], [\"date\"])\n",
    "\n",
    "# Convertir la columna \"fecha\" a tipo timestamp\n",
    "date_range_df = date_range_df.withColumn(\"date\", F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Encontrar las fechas que están en el rango pero no en df_henares\n",
    "missing_dates_df = date_range_df.subtract(df.select(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27488929-e033-4591-a4f4-16e399e255d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|date|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No faltan fechas\n",
    "missing_dates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e938640-b44d-44fe-9132-9f14974534c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por todas las columnas y contar\n",
    "duplicados = df.groupBy(df.columns).count().filter(col(\"count\") > 1)\n",
    "\n",
    "# Mostrar los duplicados\n",
    "if duplicados.count() > 0:\n",
    "    print(\"Hay valores duplicados\")\n",
    "    duplicados.show()\n",
    "else:\n",
    "    print(\"No hay valores duplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3c95e9-e49b-4bef-bb13-4439eb7a5854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-------------------+------------------+\n",
      "|summary|    temperature_2m|relative_humidity_2m|      precipitation|    wind_speed_10m|\n",
      "+-------+------------------+--------------------+-------------------+------------------+\n",
      "|  count|             17568|               17568|              17568|             17568|\n",
      "|   mean|15.099813379726596|   60.58100461316609|0.06039389799635743|10.542901945056364|\n",
      "| stddev| 8.856151760575472|  24.814841148908254| 0.3648158052589198|   5.9798685253662|\n",
      "|    min|          -11.1475|           6.2889233|                0.0|               0.0|\n",
      "|    max|           41.4025|               100.0|               22.3|         41.439106|\n",
      "+-------+------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17203711-3aae-4c0b-8ed8-5ab289af8bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------+--------------+\n",
      "|temperature_2m|relative_humidity_2m|precipitation|wind_speed_10m|\n",
      "+--------------+--------------------+-------------+--------------+\n",
      "|             0|                   0|            0|             0|\n",
      "+--------------+--------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nulos de las columnas\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns if c != \"date\"]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76ef8a9-d1ec-4bd7-bf83-4123e7300e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"datos_Madrid_horarios_transformado.csv\", header = True, mode = \"overwrite\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c32116-2a3e-41da-bc7e-78e9de626ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3369ec-57bf-4215-859c-3f840859918a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
