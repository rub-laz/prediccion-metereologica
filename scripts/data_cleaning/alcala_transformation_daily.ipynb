{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7546993-6af6-45fb-ad7e-d02bd155200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Configuración detallada para Spark\n",
    "conf = SparkConf().setAppName(\"ClusterConfigExample\") \n",
    "\n",
    "# Iniciar el SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Crear la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad3519-8bff-4d3a-adb5-e0cf2dcf0811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+-------+----+----+----+--------+----+--------+---+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|     fecha|indicativo|           nombre|provincia|altitud|tmed|prec|tmin|horatmin|tmax|horatmax|dir|velmedia|racha|horaracha|hrMedia|hrMax|horaHrMax|hrMin|horaHrMin|\n",
      "+----------+----------+-----------------+---------+-------+----+----+----+--------+----+--------+---+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|2019-01-01|     3170Y|ALCALA DE HENARES|   MADRID|    605| 5,3| 0,0|-4,1|   08:05|14,7|   15:31| 99|     0,8|  3,3|   Varias|     62|   91|    08:30|   38|    13:40|\n",
      "|2019-01-02|     3170Y|ALCALA DE HENARES|   MADRID|    605| 4,6| 0,0|-5,5|   07:04|14,6|   15:31|  8|     1,1|  3,3|    09:10|     65|   93|    08:30|   39|    15:40|\n",
      "+----------+----------+-----------------+---------+-------+----+----+----+--------+----+--------+---+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- indicativo: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- altitud: integer (nullable = true)\n",
      " |-- tmed: string (nullable = true)\n",
      " |-- prec: string (nullable = true)\n",
      " |-- tmin: string (nullable = true)\n",
      " |-- horatmin: string (nullable = true)\n",
      " |-- tmax: string (nullable = true)\n",
      " |-- horatmax: string (nullable = true)\n",
      " |-- dir: integer (nullable = true)\n",
      " |-- velmedia: string (nullable = true)\n",
      " |-- racha: string (nullable = true)\n",
      " |-- horaracha: string (nullable = true)\n",
      " |-- hrMedia: integer (nullable = true)\n",
      " |-- hrMax: integer (nullable = true)\n",
      " |-- horaHrMax: string (nullable = true)\n",
      " |-- hrMin: integer (nullable = true)\n",
      " |-- horaHrMin: string (nullable = true)\n",
      "\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.csv('datos_combinados_Alcala_Henares.csv', header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487fcde-7838-4193-a595-21e93d13bd93",
   "metadata": {},
   "source": [
    "### Exploratorio\n",
    "- Eliminar las columnas que no se vayan a utilizar.\n",
    "- Cambiar el tipo de dato.\n",
    "- Tranformar las columnas pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31448d7-22c2-44b1-9469-f3eec60db48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe: todas son strings, hay que cambiar el tipo de dato a fecha y numericos\n",
    "# 1. Elimino dichas columnas por ser innecesarias\n",
    "# 2. Cambio el tipo de dato y remplazo ',' por '.'\n",
    "# 3. La columna fecha la convierto a date\n",
    "columnas = [\"indicativo\",\"provincia\",\"nombre\",\"horatmax\",\"horatmin\",\"horaracha\",\"horaHrMax\",\"horaHrMin\",\"altitud\",\"dir\",\"prec\",\"racha\",\"hrMin\",\"hrMax\"]\n",
    "df = df.drop(*columnas)\n",
    "for columna in df.columns:\n",
    "    if columna in [\"tmed\",\"tmin\",\"tmax\",\"velmedia\"]:\n",
    "        df = df.withColumn(columna, F.regexp_replace(F.col(columna), \",\", \".\").cast(\"double\"))\n",
    "    elif columna == \"fecha\":\n",
    "        df = df.withColumn(columna, F.to_date(F.col(columna), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09c145-2622-4f28-972a-06a92b34a9c4",
   "metadata": {},
   "source": [
    "- Columnas finales:\n",
    "    - Fecha\n",
    "    - Temperatura media\n",
    "    - Temperatura mínima\n",
    "    - Temperatura máxima\n",
    "    - Velocidad media del viento\n",
    "    - Humedad realtiva media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef5cb62-dd7a-4611-9a5c-f4eba10de811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+-------+\n",
      "|     fecha|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----------+----+----+----+--------+-------+\n",
      "|2019-01-01| 5.3|-4.1|14.7|     0.8|     62|\n",
      "|2019-01-02| 4.6|-5.5|14.6|     1.1|     65|\n",
      "+----------+----+----+----+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columnas del dataframe al hacer las operaciones anteriores\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb4b25f-d64c-4e63-a045-0b3b942fc37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- tmed: double (nullable = true)\n",
      " |-- tmin: double (nullable = true)\n",
      " |-- tmax: double (nullable = true)\n",
      " |-- velmedia: double (nullable = true)\n",
      " |-- hrMedia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esquema con los tipos de datos cambiados\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e9427-31dc-4537-ad64-1b44a5adf583",
   "metadata": {},
   "source": [
    "- Genero una lista de días entre dos rangos de fechas para comprobar que no falten fechas en el dataframe original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1de86e4-8c13-48eb-84b7-0de1fbb5a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     fecha|\n",
      "+----------+\n",
      "|2019-09-16|\n",
      "|2019-09-17|\n",
      "|2019-09-18|\n",
      "|2019-10-06|\n",
      "|2019-10-08|\n",
      "|2019-12-31|\n",
      "|2020-10-20|\n",
      "|2020-10-21|\n",
      "|2025-02-11|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generar rango de fechas en una lista\n",
    "# from pyspark.sql.types import DateType\n",
    "# from datetime import datetime, timedelta\n",
    "start_date = datetime(2019, 1, 1)\n",
    "end_date = datetime(2025, 2, 11)\n",
    "date_range_list = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Crear un DataFrame en PySpark con las fechas del rango\n",
    "date_range_df = spark.createDataFrame([(d,) for d in date_range_list], [\"fecha\"]).withColumn(\"fecha\", F.to_date(F.col(\"fecha\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Encontrar las fechas que están en el rango pero no en df_henares\n",
    "missing_dates_df = date_range_df.subtract(df.select(\"fecha\"))\n",
    "\n",
    "\n",
    "missing_dates_df.sort(\"fecha\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd32b2de-26d8-4dc6-8ba6-45d25e5fef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+-------+\n",
      "|     fecha|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----------+----+----+----+--------+-------+\n",
      "|2019-01-01| 5.3|-4.1|14.7|     0.8|     62|\n",
      "|2019-01-02| 4.6|-5.5|14.6|     1.1|     65|\n",
      "|2019-01-03| 6.0|-2.8|14.7|     1.1|     63|\n",
      "|2019-01-04| 5.0|-4.2|14.3|     0.6|     60|\n",
      "|2019-01-05| 4.5|-5.8|14.8|     1.1|     58|\n",
      "|2019-01-06| 4.7|-7.1|16.5|     1.4|     51|\n",
      "|2019-01-07| 6.0|-5.5|17.4|     0.8|     43|\n",
      "|2019-01-08| 5.5|-5.3|16.3|     0.8|     55|\n",
      "|2019-01-09| 5.4|-3.6|14.4|     1.9|     61|\n",
      "|2019-01-10| 2.2|-5.0| 9.5|     3.1|     60|\n",
      "|2019-01-11| 1.4|-7.1| 9.9|     2.2|     54|\n",
      "|2019-01-12| 3.2|-5.5|11.9|     1.9|     63|\n",
      "|2019-01-13| 3.3|-5.8|12.4|     0.6|     65|\n",
      "|2019-01-14| 6.6|-4.2|17.4|     0.3|     64|\n",
      "|2019-01-15| 6.2|-3.2|15.5|     0.3|     64|\n",
      "|2019-01-16| 3.7|-4.5|11.9|     1.9|     73|\n",
      "|2019-01-17| 2.8|-2.7| 8.3|     0.0|     85|\n",
      "|2019-01-18| 1.0|-5.1| 7.2|     0.3|     77|\n",
      "|2019-01-19|-0.2|-6.0| 5.6|     1.4|     85|\n",
      "|2019-01-20| 4.6|-1.3|10.5|     2.8|     72|\n",
      "+----------+----+----+----+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meter las fechas que faltan en el dataframe \n",
    "df_union = df.join(missing_dates_df, on=\"fecha\", how=\"outer\")\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc15de4-9219-41e8-bda8-21f3fbfe9694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+-------+\n",
      "|     fecha|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----------+----+----+----+--------+-------+\n",
      "|2019-09-16|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-17|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-18|NULL|NULL|NULL|    NULL|   NULL|\n",
      "+----------+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que se han metido correctamente (solo va a mostrar 3 registros)\n",
    "df_filtered = df_union.filter((F.col(\"fecha\") >= \"2019-09-16\") & (F.col(\"fecha\") <= \"2019-09-18\"))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e094cdcf-4e5f-4723-9669-1970022d35d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay valores duplicados\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por todas las columnas y contar\n",
    "duplicados = df_union.groupBy(df_union.columns).count().filter(col(\"count\") > 1)\n",
    "\n",
    "# Mostrar los duplicados\n",
    "if duplicados.count() > 0:\n",
    "    print(\"Hay valores duplicados\")\n",
    "    duplicados.show()\n",
    "else:\n",
    "    print(\"No hay valores duplicados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2a399-4d36-4d42-adbb-a4c80d1fb3eb",
   "metadata": {},
   "source": [
    "### Detección de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b51d93c-7060-4da7-8604-3fcdb133e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------+-------+\n",
      "|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----+----+----+--------+-------+\n",
      "|  52|  52|  52|      43|     43|\n",
      "+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col,isnan, when, count\n",
    "# Nulos en cada columna\n",
    "columnas = [\"tmed\",\"tmin\",\"tmax\",\"velmedia\",\"hrMedia\"]\n",
    "df_union.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columnas]\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e7cedd-9ea5-4631-8c6b-e72bc71fcc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+-------+\n",
      "|     fecha|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----------+----+----+----+--------+-------+\n",
      "|2019-09-16|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-17|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-18|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-22|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-09-23|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-10-04|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-10-05|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-10-06|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-10-07|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-10-08|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2019-12-03|NULL|NULL|NULL|     3.9|     75|\n",
      "|2019-12-30|NULL|NULL|NULL|     0.8|     76|\n",
      "|2019-12-31|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-04-28|NULL|NULL|NULL|     2.5|     70|\n",
      "|2020-04-29|NULL|NULL|NULL|     5.0|     65|\n",
      "|2020-04-30|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-05-01|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-05-02|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-05-03|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-05-04|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-05-05|NULL|NULL|NULL|     2.8|     47|\n",
      "|2020-09-20|NULL|NULL|NULL|     3.1|     80|\n",
      "|2020-09-21|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-22|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-23|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-24|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-25|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-26|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-27|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-28|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-29|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-09-30|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-01|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-02|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-03|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-15|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-16|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-17|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-18|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-19|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-20|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-21|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-22|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-23|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-24|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-25|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-26|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2020-10-27|NULL|NULL|NULL|    NULL|   NULL|\n",
      "|2021-08-11|NULL|NULL|NULL|     3.6|     37|\n",
      "|2022-11-29|NULL|NULL|NULL|     0.6|     74|\n",
      "|2024-08-14|NULL|NULL|NULL|     3.6|     47|\n",
      "|2025-02-11|NULL|NULL|NULL|    NULL|   NULL|\n",
      "+----------+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Muestra de los registros que tienen tmin nulo, coincide que aquí se concentran la mayoría de nulos\n",
    "df_union.filter(F.col(\"tmin\").isNull()).show(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f60fd3-85f7-49d3-bdf1-8bbba7e3d727",
   "metadata": {},
   "source": [
    "## IMPUTACIÓN DE NULOS\n",
    "- Lo que se va a intentar es si detecta nulos coger el valor anterior que no sea nulo e imputarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea031ff-a8b1-486c-8464-4e57834855ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "# import sys\n",
    "# ---------------------------------------------------TMED---------------------------------------------------------------\n",
    "# Definir una ventana ordenada por fecha\n",
    "window_spec = Window.orderBy(\"fecha\")\n",
    "\n",
    "# Obtener el valor anterior no nulo\n",
    "window_prev = window_spec.rowsBetween(-sys.maxsize, -1)\n",
    "df_prueba = df_union.withColumn(\"prev_tmed\", F.last(F.col(\"tmed\"), ignorenulls=True).over(window_prev))\n",
    "\n",
    "# Obtener el valor siguiente no nulo\n",
    "window_next = window_spec.rowsBetween(1, sys.maxsize)\n",
    "df_prueba = df_prueba.withColumn(\"next_tmed\", F.first(F.col(\"tmed\"), ignorenulls=True).over(window_next))\n",
    "\n",
    "# Calcular la media entre el valor anterior y el siguiente\n",
    "df_prueba = df_prueba.withColumn(\"imputed_tmed\", (F.col(\"prev_tmed\") + F.col(\"next_tmed\")) / 2)\n",
    "\n",
    "# Rellenar solo los valores nulos en 'tmed' con la media calculada\n",
    "df_prueba = df_prueba.withColumn(\"tmed\", F.when(F.col(\"tmed\").isNull(), F.col(\"imputed_tmed\")).otherwise(F.col(\"tmed\")))\n",
    "\n",
    "# Eliminar las columnas auxiliares ('prev_tmed', 'next_tmed', 'imputed_tmed')\n",
    "df_prueba = df_prueba.drop(\"prev_tmed\", \"next_tmed\", \"imputed_tmed\")\n",
    "\n",
    "# ---------------------------------------------------TMIN---------------------------------------------------------------\n",
    "# Definir una ventana ordenada por fecha\n",
    "window_spec = Window.orderBy(\"fecha\")\n",
    "\n",
    "# Obtener el valor anterior no nulo\n",
    "window_prev = window_spec.rowsBetween(-sys.maxsize, -1)\n",
    "df_prueba = df_prueba.withColumn(\"prev_tmin\", F.last(F.col(\"tmin\"), ignorenulls=True).over(window_prev))\n",
    "\n",
    "# Obtener el valor siguiente no nulo\n",
    "window_next = window_spec.rowsBetween(1, sys.maxsize)\n",
    "df_prueba = df_prueba.withColumn(\"next_tmin\", F.first(F.col(\"tmin\"), ignorenulls=True).over(window_next))\n",
    "\n",
    "# Calcular la media entre el valor anterior y el siguiente\n",
    "df_prueba = df_prueba.withColumn(\"imputed_tmin\", (F.col(\"prev_tmin\") + F.col(\"next_tmin\")) / 2)\n",
    "\n",
    "# Rellenar solo los valores nulos en 'tmed' con la media calculada\n",
    "df_prueba = df_prueba.withColumn(\"tmin\", F.when(F.col(\"tmin\").isNull(), F.col(\"imputed_tmin\")).otherwise(F.col(\"tmin\")))\n",
    "\n",
    "# Eliminar las columnas auxiliares ('prev_tmed', 'next_tmed', 'imputed_tmed')\n",
    "df_prueba = df_prueba.drop(\"prev_tmin\", \"next_tmin\", \"imputed_tmin\")\n",
    "\n",
    "# ---------------------------------------------------TMAX---------------------------------------------------------------\n",
    "# Definir una ventana ordenada por fecha\n",
    "window_spec = Window.orderBy(\"fecha\")\n",
    "\n",
    "# Obtener el valor anterior no nulo\n",
    "window_prev = window_spec.rowsBetween(-sys.maxsize, -1)\n",
    "df_prueba = df_prueba.withColumn(\"prev_tmax\", F.last(F.col(\"tmax\"), ignorenulls=True).over(window_prev))\n",
    "\n",
    "# Obtener el valor siguiente no nulo\n",
    "window_next = window_spec.rowsBetween(1, sys.maxsize)\n",
    "df_prueba = df_prueba.withColumn(\"next_tmax\", F.first(F.col(\"tmax\"), ignorenulls=True).over(window_next))\n",
    "\n",
    "# Calcular la media entre el valor anterior y el siguiente\n",
    "df_prueba = df_prueba.withColumn(\"imputed_tmax\", (F.col(\"prev_tmax\") + F.col(\"next_tmax\")) / 2)\n",
    "\n",
    "# Rellenar solo los valores nulos en 'tmed' con la media calculada\n",
    "df_prueba = df_prueba.withColumn(\"tmax\", F.when(F.col(\"tmax\").isNull(), F.col(\"imputed_tmax\")).otherwise(F.col(\"tmax\")))\n",
    "\n",
    "# Eliminar las columnas auxiliares ('prev_tmed', 'next_tmed', 'imputed_tmed')\n",
    "df_prueba = df_prueba.drop(\"prev_tmax\", \"next_tmax\", \"imputed_tmax\")\n",
    "\n",
    "# ---------------------------------------------------VELMEDIA---------------------------------------------------------------\n",
    "# Definir una ventana ordenada por fecha\n",
    "window_spec = Window.orderBy(\"fecha\")\n",
    "\n",
    "# Obtener el valor anterior no nulo\n",
    "window_prev = window_spec.rowsBetween(-sys.maxsize, -1)\n",
    "df_prueba = df_prueba.withColumn(\"prev_velmedia\", F.last(F.col(\"velmedia\"), ignorenulls=True).over(window_prev))\n",
    "\n",
    "# Obtener el valor siguiente no nulo\n",
    "window_next = window_spec.rowsBetween(1, sys.maxsize)\n",
    "df_prueba = df_prueba.withColumn(\"next_velmedia\", F.first(F.col(\"velmedia\"), ignorenulls=True).over(window_next))\n",
    "\n",
    "# Calcular la media entre el valor anterior y el siguiente\n",
    "df_prueba = df_prueba.withColumn(\"imputed_velmedia\", (F.col(\"prev_velmedia\") + F.col(\"next_velmedia\")) / 2)\n",
    "\n",
    "# Rellenar solo los valores nulos en 'tmed' con la media calculada\n",
    "df_prueba = df_prueba.withColumn(\"velmedia\", F.when(F.col(\"velmedia\").isNull(), F.col(\"imputed_velmedia\")).otherwise(F.col(\"velmedia\")))\n",
    "\n",
    "# Eliminar las columnas auxiliares ('prev_tmed', 'next_tmed', 'imputed_tmed')\n",
    "df_prueba = df_prueba.drop(\"prev_velmedia\", \"next_velmedia\", \"imputed_velmedia\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------HRMEDIA---------------------------------------------------------------\n",
    "# Definir una ventana ordenada por fecha\n",
    "window_spec = Window.orderBy(\"fecha\")\n",
    "\n",
    "# Obtener el valor anterior no nulo\n",
    "window_prev = window_spec.rowsBetween(-sys.maxsize, -1)\n",
    "df_prueba = df_prueba.withColumn(\"prev_hrMedia\", F.last(F.col(\"hrMedia\"), ignorenulls=True).over(window_prev))\n",
    "\n",
    "# Obtener el valor siguiente no nulo\n",
    "window_next = window_spec.rowsBetween(1, sys.maxsize)\n",
    "df_prueba = df_prueba.withColumn(\"next_hrMedia\", F.first(F.col(\"hrMedia\"), ignorenulls=True).over(window_next))\n",
    "\n",
    "# Calcular la media entre el valor anterior y el siguiente\n",
    "df_prueba = df_prueba.withColumn(\"imputed_hrMedia\", (F.col(\"prev_hrMedia\") + F.col(\"next_hrMedia\")) / 2)\n",
    "\n",
    "# Rellenar solo los valores nulos en 'tmed' con la media calculada\n",
    "df_prueba = df_prueba.withColumn(\"hrMedia\", F.when(F.col(\"hrMedia\").isNull(), F.col(\"imputed_hrMedia\")).otherwise(F.col(\"hrMedia\")))\n",
    "\n",
    "# Eliminar las columnas auxiliares ('prev_tmed', 'next_tmed', 'imputed_tmed')\n",
    "df_prueba = df_prueba.drop(\"prev_hrMedia\", \"next_hrMedia\", \"imputed_hrMedia\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b5b7e4-b545-40ff-ba1b-25560f403c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------+-------+\n",
      "|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----+----+----+--------+-------+\n",
      "|   1|   1|   1|       1|      1|\n",
      "+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nulos en cada columna\n",
    "columnas = [\"tmed\",\"tmin\",\"tmax\",\"velmedia\",\"hrMedia\"]\n",
    "df_prueba.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columnas]\n",
    "   ).show()\n",
    "# df_prueba.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa8cbda-f0ce-4cd0-b8d7-0acb9ccd9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+-------+\n",
      "|     fecha|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----------+----+----+----+--------+-------+\n",
      "|2025-02-11|NULL|NULL|NULL|    NULL|   NULL|\n",
      "+----------+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Es el ultimo registro\n",
    "df_prueba.filter(F.col(\"tmed\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8982a1-2981-40bd-b5e2-1a2fe69cd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba = df_prueba.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd08252-cb5a-4236-9c1d-39f4896a9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------+-------+\n",
      "|tmed|tmin|tmax|velmedia|hrMedia|\n",
      "+----+----+----+--------+-------+\n",
      "|   0|   0|   0|       0|      0|\n",
      "+----+----+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ya no hay nulos\n",
    "columnas = [\"tmed\",\"tmin\",\"tmax\",\"velmedia\",\"hrMedia\"]\n",
    "df_prueba.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columnas]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "258a4b08-2ee5-4855-ab76-b95d84da63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba.write.csv(\"datos_Alcala_Henares_transformado.csv\", header = True, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d646c5c-8037-4698-b2e6-c6f806fe5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d6c4a-d048-40cc-b820-c4412507a296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
